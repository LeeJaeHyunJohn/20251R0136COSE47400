\section{Introduction}
\label{sec:intro}

Neural networks have become a fundamental tool in modern machine learning, particularly in the field of image classification. In this project, we explore the design and implementation of a simple two-layer fully connected neural network (multi-layer perceptron, or MLP) trained on the MNIST dataset. Our objective is not only to implement the core components of the network manually—including forward and backward passes, loss computation using softmax, and L2 regularization—but also to gain practical understanding through hyperparameter tuning.

We begin by testing our network on toy data to verify the correctness of the forward and backward computations using numerical gradient checking. Once validated, we train the network on MNIST, a dataset of handwritten digits, and observe how hyperparameters such as learning rate, hidden layer size, regularization strength, and number of training iterations affect performance. Through systematic experimentation, we aim to find a good combination of parameters that leads to high classification accuracy without overfitting.

By visualizing both the training dynamics (loss and accuracy trends) and the learned weights, we develop deeper insight into the learning process. Ultimately, we achieve over 90\% accuracy on the test set, confirming the capability of shallow networks in simple image classification tasks. This hands-on approach enhances our understanding of the building blocks of neural networks and highlights the impact of each design choice.


