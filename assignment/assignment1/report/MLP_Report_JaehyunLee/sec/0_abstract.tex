\begin{abstract}
    In this project, I implement a two-layer neural network consisting of an input layer, a hidden layer with ReLU activation, and an output layer with softmax loss. 
    The model is trained on the MNIST dataset using stochastic gradient descent (SGD) and $L_2$ regularization. 
    We investigate the impact of various hyperparameters, such as learning rate, hidden layer size, regularization strength, and number of training iterations. 
    By tuning these parameters and analyzing loss/accuracy trends and weight visualizations, we achieved a validation accuracy above 91\% and a test accuracy of 91.3\%. 
    This demonstrates the effectiveness of a simple MLP in image classification when properly tuned.    
\end{abstract}