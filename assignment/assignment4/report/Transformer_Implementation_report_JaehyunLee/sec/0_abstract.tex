\begin{abstract}
        This paper presents the implementation and evaluation of a Transformer-based text classification model on the IMDb movie review dataset. We construct the Transformer architecture from scratch, including positional encoding and multi-head attention mechanisms, to classify movie reviews into positive or negative sentiments. 
        Additionally, we fine-tune a pre-trained BERT model on the same dataset to compare performance. Our results demonstrate that while the self-implemented Transformer performs reasonably well, the BERT-based model achieves significantly higher accuracy due to its deep contextual understanding. 
        This work highlights the importance of pre-training in modern NLP models and provides insights into the internal workings of Transformer architectures.
\end{abstract}