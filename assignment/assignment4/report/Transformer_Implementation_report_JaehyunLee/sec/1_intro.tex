\section{Introduction}
\label{sec:intro}

Natural Language Processing (NLP) has undergone a major paradigm shift with the introduction of Transformer-based architectures. Unlike Recurrent Neural Networks (RNNs), Transformers utilize self-attention mechanisms to capture long-range dependencies in textual data more efficiently and in parallel. This architecture forms the foundation of many state-of-the-art language models such as BERT, GPT, and T5.

In this work, we aim to implement a Transformer-based classifier from scratch to perform binary sentiment classification on the IMDb movie review dataset. The primary objective is to understand the inner mechanisms of the Transformer model, including positional encoding, multi-head attention, and encoder layers. To evaluate the effectiveness of our implementation, we also fine-tune a pre-trained BERT model on the same dataset and compare its performance with our self-built model.

Through this assignment, we highlight the performance gap between pre-trained language models and models trained from scratch. Our results emphasize the benefits of large-scale pre-training and demonstrate how Transformer components interact to process and classify natural language.