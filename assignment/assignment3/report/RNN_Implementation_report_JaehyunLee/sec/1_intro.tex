\section{Introduction}
\label{sec:intro}

Recurrent Neural Networks (RNNs) have become a standard architecture for modeling sequential data in natural language processing, time series prediction, and speech recognition. Unlike feedforward neural networks, RNNs maintain a hidden state across time steps, allowing them to learn temporal dependencies.

However, vanilla RNNs suffer from the vanishing gradient problem, which limits their ability to capture long-term dependencies. To overcome this, more advanced architectures such as the Gated Recurrent Unit (GRU) and Long Short-Term Memory (LSTM) were introduced. These models incorporate gating mechanisms that help regulate the flow of information and gradients through time.

In this project, we implemented both a vanilla RNN and a GRU from scratch using PyTorch. We trained and evaluated these models on a sequence classification task. The goal of this assignment was not only to achieve high classification accuracy, but also to understand the inner workings of recurrent models by manually coding forward and backward passes.



